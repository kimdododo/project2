services:
  # MySQL 데이터베이스
  mysql:
    image: mysql:8.0
    container_name: project-mysql
    command: ["--default-authentication-plugin=mysql_native_password","--character-set-server=utf8mb4","--collation-server=utf8mb4_0900_ai_ci","--sql-require-primary-key=ON"]
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: yt
      MYSQL_USER: ytuser
      MYSQL_PASSWORD: ytpw
      TZ: Asia/Seoul
    ports:
      - "3307:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping -h localhost -proot --silent"]
      interval: 5s
      timeout: 5s
      retries: 20

  # PostgreSQL (Airflow용)
  airflow-db:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_db:/var/lib/postgresql/data

  # 백엔드 (FastAPI)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: project-backend
    ports:
      - "8000:8000"
    environment:
      - MYSQL_HOST=mysql
      - MYSQL_PORT=3306
      - MYSQL_DB=yt
      - MYSQL_USER=ytuser
      - MYSQL_PW=ytpw
      - YOUTUBE_API_KEY=${YOUTUBE_API_KEY}
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_APP_NAME=youtube-analytics
    depends_on:
      mysql:
        condition: service_healthy
      kafka:
        condition: service_started
      spark-master:
        condition: service_started
    volumes:
      - ./backend:/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # 프론트엔드 (Next.js)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: project-frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NEXT_PUBLIC_SUPABASE_URL=https://axyzrvlfcpysyqonqvhq.supabase.co
      - NEXT_PUBLIC_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImF4eXpydmxmY3B5c3lxb25xdmhxIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjAxODIwNzYsImV4cCI6MjA3NTc1ODA3Nn0.-9RXGg_8EUQI92GLqscW0qMzstjEcJjjtIMY2deRpas
      - SUPABASE_SERVICE_ROLE_KEY=your-supabase-service-role-key
    depends_on:
      - backend
    volumes:
      - ./frontend:/app
      - /app/node_modules
    command: npm run dev

  # Airflow 웹서버
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: airflow-ml:latest
    command: webserver
    env_file:
      - ./.env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      PYTHONPATH: /opt/airflow/dags
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/requirements.txt:/requirements.txt
    depends_on:
      - airflow-db
      - mysql

  # Airflow 스케줄러
  airflow-scheduler:
    image: airflow-ml:latest
    command: scheduler
    env_file:
      - ./.env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      PYTHONPATH: /opt/airflow/dags
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/requirements.txt:/requirements.txt
    depends_on:
      - airflow-db
      - mysql

  # Airflow 초기화
  airflow-init:
    image: airflow-ml:latest
    entrypoint: bash -c "airflow db init && airflow users create --username admin --password admin --firstname a --lastname a --role Admin --email a@a.a && /bin/true"
    env_file:
      - ./.env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
    volumes:
      - ./airflow/requirements.txt:/requirements.txt
    depends_on:
      - airflow-db
      - mysql

  # Zookeeper (Kafka 의존성)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: project-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: project-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
    volumes:
      - kafka_data:/var/lib/kafka/data

  # Kafka UI (선택사항 - Kafka 관리용 웹 인터페이스)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: project-kafka-ui
    depends_on:
      - kafka
    ports:
      - "8081:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092

  # Spark Master
  spark-master:
    image: apache/spark:3.4.0
    container_name: project-spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "0.0.0.0", "--port", "7077", "--webui-port", "8080"]
    ports:
      - "8083:8080"  # Spark Master UI
      - "7077:7077"  # Spark Master port
    volumes:
      - spark_data:/tmp/spark-events

  # Spark Worker 1
  spark-worker-1:
    image: apache/spark:3.4.0
    container_name: project-spark-worker-1
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--webui-port", "8081"]
    ports:
      - "8084:8081"  # Spark Worker UI
    volumes:
      - spark_data:/tmp/spark-events

  # Spark Worker 2
  spark-worker-2:
    image: apache/spark:3.4.0
    container_name: project-spark-worker-2
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--webui-port", "8081"]
    ports:
      - "8085:8081"  # Spark Worker UI
    volumes:
      - spark_data:/tmp/spark-events

  # Jupyter Notebook with Spark (선택사항 - 개발용)
  jupyter-spark:
    image: jupyter/pyspark-notebook:latest
    container_name: project-jupyter-spark
    depends_on:
      - spark-master
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./notebooks:/home/jovyan/work
      - spark_data:/tmp/spark-events
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''

volumes:
  mysql_data: {}
  airflow_db: {}
  zookeeper_data: {}
  zookeeper_logs: {}
  kafka_data: {}
  spark_data: {}
